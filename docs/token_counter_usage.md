# Token Counter ä½¿ã„æ–¹ã‚¬ã‚¤ãƒ‰

`TiktokenCountCallback` ã¯ã€LangChain ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® LLM å‘¼ã³å‡ºã—ã‚’è¿½è·¡ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã™ã€‚

## åŸºæœ¬çš„ãªä½¿ã„æ–¹

### 1. åˆæœŸåŒ–

```python
from appium_tools.token_counter import TiktokenCountCallback

# ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®šã—ã¦åˆæœŸåŒ–
token_counter = TiktokenCountCallback(model="gpt-4.1")
```

ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«:
- GPT-5ã‚·ãƒªãƒ¼ã‚º: `gpt-5`, `gpt-5-mini`, `gpt-5-nano`, `gpt-5-pro`
- GPT-4.1ã‚·ãƒªãƒ¼ã‚º: `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`
- GPT-4oã‚·ãƒªãƒ¼ã‚º: `gpt-4o`, `gpt-4o-mini`
- O-ã‚·ãƒªãƒ¼ã‚º: `o1`, `o1-mini`, `o3`, `o3-mini`, `o4-mini`
- ãã®ä»–å¤šæ•°ï¼ˆè©³ç´°ã¯ `token_counter.py` å‚ç…§ï¼‰

### 2. LangChain ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è¨­å®š

```python
from langchain_core.runnables import RunnableConfig

response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": user_input}]},
    config=RunnableConfig(
        callbacks=[token_counter]  # ã“ã“ã§è¨­å®š
    )
)
```

## æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³: track_query() ã‚’ä½¿ç”¨

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§æ¨å¥¨ã•ã‚Œã‚‹ä½¿ã„æ–¹ã§ã™ã€‚

```python
from appium_tools.token_counter import TiktokenCountCallback

# åˆæœŸåŒ–
token_counter = TiktokenCountCallback(model="gpt-4.1")

# ã‚¯ã‚¨ãƒªã”ã¨ã«è¿½è·¡
with token_counter.track_query() as query:
    # LLMå‘¼ã³å‡ºã—
    response = await agent.ainvoke(
        {"messages": [{"role": "user", "content": user_input}]},
        config=RunnableConfig(callbacks=[token_counter])
    )
    
    # ã“ã®ã‚¯ã‚¨ãƒªã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    print(query.report())

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
print(token_counter.format_session_summary())
```

### track_query() ã®åˆ©ç‚¹

- âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ãŒä¸è¦ï¼ˆè‡ªå‹•ï¼‰
- âœ… ã‚¹ã‚³ãƒ¼ãƒ—ãŒæ˜ç¢ºï¼ˆ`with` ãƒ–ãƒ­ãƒƒã‚¯ï¼‰
- âœ… ã‚¨ãƒ©ãƒ¼ãŒå°‘ãªã„
- âœ… èª­ã¿ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰

## å®Œå…¨ãªä½¿ç”¨ä¾‹ï¼ˆchat.py ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

```python
import asyncio
from langchain.agents import create_agent
from langchain_core.runnables import RunnableConfig
from appium_tools.token_counter import TiktokenCountCallback

async def main():
    # åˆæœŸåŒ–
    token_counter = TiktokenCountCallback(model="gpt-4.1")
    agent = create_agent(model="gpt-4.1", tools=tools)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    while True:
        user_input = input("You: ").strip()
        
        if user_input.lower() in ['quit', 'exit']:
            break
        
        # ã‚¯ã‚¨ãƒªã‚’è¿½è·¡
        with token_counter.track_query() as query:
            response = await agent.ainvoke(
                {"messages": [{"role": "user", "content": user_input}]},
                config=RunnableConfig(callbacks=[token_counter])
            )
            
            print(f"\nAssistant: {response['messages'][-1].content}\n")
            
            # ã“ã®ã‚¯ã‚¨ãƒªã®ãƒ¬ãƒãƒ¼ãƒˆ
            report = query.report()
            if report:
                print(report)
    
    # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã€ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼
    session_summary = token_counter.format_session_summary()
    if session_summary:
        print("\n" + session_summary + "\n")

if __name__ == '__main__':
    asyncio.run(main())
```

### å‡ºåŠ›ä¾‹

**ã‚¯ã‚¨ãƒªã”ã¨ï¼ˆãƒ«ãƒ¼ãƒ—å†…ï¼‰:**
```
======================================================================
ğŸ“Š This Query LLM Calls:
======================================================================

ğŸ”¹ Call #1 (1.23s)
   Model: gpt-4.1
   Tokens: 1500 input + 200 output = 1700 total
   ğŸ’° Cost: $0.004600

ğŸ”¹ Call #2 (0.95s)
   Model: gpt-4.1
   Tokens: 1800 input + 150 output = 1950 total
   ğŸ’¾ Cache Hit: 500 tokens saved $0.000250
   ğŸ’° Cost: $0.004200

----------------------------------------------------------------------
ğŸ“Š This Query Total: 2 calls, 3650 tokens, $0.008800
======================================================================
```

**ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚:**
```
======================================================================
ğŸ“ˆ SESSION SUMMARY:
======================================================================
Total LLM Calls: 6
Total Tokens: 10850 (9200 input + 1650 output)
ğŸ’¾ Total Cached: 1200 tokens
ğŸ’° Total Cost: $0.025400
ğŸ“Š Average: 1808.3 tokens/call, $0.004233/call
======================================================================
```

## ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ¡ã‚½ãƒƒãƒ‰

### å±¥æ­´å–å¾—

```python
# å…¨å±¥æ­´ã‚’å–å¾—
history = token_counter.get_invocation_history()
# [
#   {
#     "invocation_id": 1,
#     "timestamp": "2025-11-28T10:30:45.123456",
#     "elapsed_seconds": 1.23,
#     "model": "gpt-4.1",
#     "input_tokens": 1500,
#     "cached_tokens": 0,
#     "output_tokens": 200,
#     "total_tokens": 1700,
#     "input_cost_usd": 0.003000,
#     "output_cost_usd": 0.001600,
#     "cached_cost_usd": 0.0,
#     "total_cost_usd": 0.004600
#   },
#   ...
# ]

# ç‰¹å®šã®IDã§å–å¾—
inv = token_counter.get_invocation_by_id(2)

# æœ€æ–°ã®å‘¼ã³å‡ºã—ã‚’å–å¾—
latest = token_counter.get_latest_invocation()
```

### ã‚µãƒãƒªãƒ¼å–å¾—

```python
summary = token_counter.get_invocations_summary()
# {
#   "total_invocations": 6,
#   "total_input_tokens": 9200,
#   "total_cached_tokens": 1200,
#   "total_output_tokens": 1650,
#   "total_tokens": 10850,
#   "total_cost_usd": 0.025400,
#   "average_tokens_per_invocation": 1808.33,
#   "average_cost_per_invocation": 0.004233
# }
```

### å¾Œæ–¹äº’æ›æ€§ï¼ˆå¾“æ¥ã®ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰

```python
# ç´¯ç©ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã•ã‚Œã¦ã„ã¾ã™ï¼‰
metrics = token_counter.get_metrics()
# {
#   "model": "gpt-4.1",
#   "input_tokens": 9200,
#   "cached_tokens": 1200,
#   "output_tokens": 1650,
#   "total_tokens": 10850,
#   "input_cost_usd": 0.018400,
#   "output_cost_usd": 0.013200,
#   "cached_cost_usd": 0.000600,
#   "total_cost_usd": 0.031400
# }
```

## ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿å‡ºåŠ›

### è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ

```python
# å…¨invocationã®è©³ç´°
print(token_counter.format_invocation_details())

# ã‚µãƒãƒªãƒ¼ã®ã¿
print(token_counter.format_summary())

# ä¸¡æ–¹ï¼ˆè©³ç´° + ã‚µãƒãƒªãƒ¼ï¼‰
print(token_counter.format_report())

# ã‚µãƒãƒªãƒ¼ã®ã¿ï¼ˆè©³ç´°ãªã—ï¼‰
print(token_counter.format_report(show_details=False))
```

### ã‚«ã‚¹ã‚¿ãƒ å¹…

```python
# è¡¨ç¤ºå¹…ã‚’æŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
print(token_counter.format_summary(width=80))
print(token_counter.format_session_summary(width=100))
```

## ãƒªã‚»ãƒƒãƒˆ

```python
# å…¨ã¦ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã¨å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
token_counter.reset_counters()

# ãƒªã‚»ãƒƒãƒˆå¾Œã¯ invocation_id ãŒ 1 ã‹ã‚‰å†é–‹
```

## é«˜åº¦ãªä½¿ã„æ–¹

### æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ï¼ˆéæ¨å¥¨ï¼‰

`track_query()` ã‚’ä½¿ã‚ãªã„å ´åˆ:

```python
# é–‹å§‹æ™‚ç‚¹ã‚’è¨˜éŒ²
start_index = len(token_counter.get_invocation_history())

# LLMå‘¼ã³å‡ºã—
response = await agent.ainvoke(...)

# ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
print(token_counter.format_loop_report(start_index))
```

**æ³¨æ„:** ã“ã®æ–¹æ³•ã¯ç…©é›‘ãªã®ã§ã€`track_query()` ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®é€”ä¸­å¤‰æ›´

```python
# ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ã¦ã‚‚ã€å„invocationã«æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ã‚¹ãƒˆãŒè¨˜éŒ²ã•ã‚Œã‚‹
token_counter.model = "gpt-4o-mini"
response1 = await agent.ainvoke(...)  # gpt-4o-mini ã¨ã—ã¦è¨˜éŒ²

token_counter.model = "gpt-4.1"
response2 = await agent.ainvoke(...)  # gpt-4.1 ã¨ã—ã¦è¨˜éŒ²

# å„invocationã®modelãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ç¢ºèªå¯èƒ½
history = token_counter.get_invocation_history()
print(history[0]["model"])  # "gpt-4o-mini"
print(history[1]["model"])  # "gpt-4.1"
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### âœ… æ¨å¥¨

```python
# 1. track_query() ã‚’ä½¿ã†
with token_counter.track_query() as query:
    response = await agent.ainvoke(...)
    print(query.report())

# 2. ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ã‚µãƒãƒªãƒ¼è¡¨ç¤º
print(token_counter.format_session_summary())

# 3. é•·æœŸé–“ä½¿ç”¨ã™ã‚‹å ´åˆã¯å®šæœŸçš„ã«ãƒªã‚»ãƒƒãƒˆ
if iteration % 100 == 0:
    token_counter.reset_counters()
```

### âŒ éæ¨å¥¨

```python
# æ‰‹å‹•ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ï¼ˆé¢å€’ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã‚„ã™ã„ï¼‰
start = len(token_counter.get_invocation_history())
response = await agent.ainvoke(...)
print(token_counter.format_loop_report(start))

# ç´¯ç©ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ã¿ä½¿ç”¨ï¼ˆè©³ç´°ãŒå¤±ã‚ã‚Œã‚‹ï¼‰
metrics = token_counter.get_metrics()
print(metrics["total_cost_usd"])
```

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨˜éŒ²ã•ã‚Œãªã„

OpenAI API ãŒ `prompt_tokens_details.cached_tokens` ã‚’è¿”ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãŒç„¡åŠ¹ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### ã‚³ã‚¹ãƒˆãŒæ­£ç¢ºã§ãªã„

ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«åã¯ `token_counter.py` ã® `PRICING` è¾æ›¸ã«å«ã¾ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### å±¥æ­´ãŒè“„ç©ã•ã‚Œã™ãã‚‹

é•·æ™‚é–“å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€å®šæœŸçš„ã« `reset_counters()` ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„:

```python
# 100ã‚¯ã‚¨ãƒªã”ã¨ã«ãƒªã‚»ãƒƒãƒˆ
if query_count % 100 == 0:
    # ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆ
    final_summary = token_counter.get_invocations_summary()
    save_to_database(final_summary)
    token_counter.reset_counters()
```

## ã¾ã¨ã‚

- **åŸºæœ¬**: `TiktokenCountCallback(model="...")` ã§åˆæœŸåŒ–
- **æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³**: `with token_counter.track_query() as query:`
- **ã‚¯ã‚¨ãƒªã”ã¨**: `query.report()` ã§è©³ç´°è¡¨ç¤º
- **ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†**: `token_counter.format_session_summary()` ã§åˆè¨ˆè¡¨ç¤º
- **ãƒ‡ãƒ¼ã‚¿å–å¾—**: `get_invocation_history()`, `get_invocations_summary()`
- **ãƒªã‚»ãƒƒãƒˆ**: `reset_counters()` ã§å±¥æ­´ã‚¯ãƒªã‚¢

ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ãˆã°ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤å¼·åŠ›ãªãƒˆãƒ¼ã‚¯ãƒ³è¿½è·¡ãŒå®Ÿç¾ã§ãã¾ã™ã€‚
